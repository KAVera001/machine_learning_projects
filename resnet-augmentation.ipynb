{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "cstaa2ft9i6i3ld0zi3fid"
   },
   "source": [
    "Мы будем предсказывать расположение и класс дефектов, обнаруженных на производстве стальных листов. Изображения имеют уникальные названия ImageId. Целью является сегментировать и классифицировать дефекты по изображениям из test датасета.<br>\n",
    "Каждое изображение может вообще не иметь дефектов, иметь дефект только одного класса или дефекты нескольких классов. Для каждого изображения нужно сегментировать дефекты каждого класса (ClassId = [1, 2, 3, 4])<br>\n",
    "Сегмент для дефекта каждого класса должен быть записан в отдельную строку, даже если на изображении присутствуют несколько дискретных расположений дефекта\n",
    "\n",
    "\n",
    "**Файлы** <br>\n",
    "- train_images/ - папка изображений для тренировки модели <br>\n",
    "- test_images/ - папка изображений для тестирования модели (мы сегментируем и классифицируем эти изображения)<br>\n",
    "- train.csv - аннотации с сегментами дефектов на изображениях из тренировочного датасета (ClassId = [1, 2, 3, 4])<br>\n",
    "- sample_submission.csv - a sample submission file in the correct format; <br>\n",
    "\n",
    "\n",
    "*note, each ImageId 4 rows, one for each of the 4 defect classes*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "f5qogo8nojquq0zy7rjnhd"
   },
   "source": [
    "Пиксели номируются сверху вниз, потом слева направо, то есть 1й пиксель это $(0,0)$, 2й пиксель это $(1,0)$.<br>\n",
    "То есть если на картинке пиксель $(i,j)$, где $i$ - номер строки, $j$ - номер столбца, то его номер $n$ вычисляется по формуле $n = 256* j + i + 1$<br>\n",
    "Формула для вычисления i, j по порядковому номеру n:<br>\n",
    "$j = int(n/256)$<br>\n",
    "$i = n - 256*int(n/256) - 1 = n - 256*j -1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellId": "bkg2c5ukz261ioeb0788m6"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellId": "z605m376wyjc2hp00pwnbu"
   },
   "outputs": [],
   "source": [
    "#%pip install opencv-python\n",
    "#%pip install plotly\n",
    "#%pip install -U protobuf==3.11.3\n",
    "#%pip install seaborn\n",
    "#%pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "qnayvab2gtoqto6epwjkhn"
   },
   "outputs": [],
   "source": [
    "#%pip install tensorflow h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "uef41dgtu2ixxlk55k24x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 14:34:36.023878: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import matplotlib.pyplot as plt # Import matplotlib for data visualisation\n",
    "import seaborn as sns\n",
    "#import pandas_profiling as pp\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import plotly\n",
    "from plotly import graph_objects as go\n",
    "import plotly.express as px\n",
    "from keras.utils import Sequence\n",
    "from skimage.io import imread\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import albumentations as aug\n",
    "from albumentations import (Blur, Compose, HorizontalFlip, HueSaturationValue,\n",
    "                            IAAEmboss, IAASharpen, JpegCompression, OneOf,\n",
    "                            RandomBrightness, RandomBrightnessContrast,\n",
    "                            RandomContrast, RandomCrop, RandomGamma,\n",
    "                            RandomRotate90, RGBShift, ShiftScaleRotate,\n",
    "                            GaussNoise,\n",
    "                            Transpose, VerticalFlip, ElasticTransform, GridDistortion, OpticalDistortion)\n",
    "from albumentations import Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "ocnnxs1ki3t0f66ach13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_images', 'train.csv', 'train_images', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "yxvv04kw4exelesc9ypn"
   },
   "outputs": [],
   "source": [
    "data_dir =\"data\"\n",
    "input_file0 = \"train.csv\"\n",
    "input_file1 = \"sample_submission.csv\"\n",
    "abspath='/'.join(os.getcwd().split('\\\\')) \n",
    "source_folder = os.path.join(abspath, 'data')\n",
    "df_train = pd.read_csv(os.path.join(source_folder,input_file0))\n",
    "df_sample = pd.read_csv(os.path.join(source_folder,input_file1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "24njbxpg0bssg6u2jbrwyb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7095, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "2zp8lbozy1nweob098ag1p"
   },
   "outputs": [],
   "source": [
    "# Пути на папки с train_images и test_images соответственно\n",
    "trainImgPath =os.path.join(source_folder,'train_images/')\n",
    "testImgPath = os.path.join(source_folder,'test_images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "d4xju194imwoy4ff4fajij"
   },
   "outputs": [],
   "source": [
    "# Создадим таблицу с ImageId из папки train_images\n",
    "# Каждому ImageId будут соответствовать 4 строки (для каждого класса дефекта)\n",
    "train_Img_Id = []\n",
    "train_class_Id = []\n",
    "for i in os.listdir(trainImgPath):\n",
    "    for j in range(1,5):\n",
    "        train_Img_Id.append(i)\n",
    "        train_class_Id.append(j)\n",
    "train_Imgs = pd.DataFrame(train_Img_Id,columns=['ImageId'])\n",
    "train_Imgs['ClassId'] = train_class_Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "27ojnfuitu1rhwva5d7aem"
   },
   "outputs": [],
   "source": [
    "# Создадим таблицу - объединение 2х таблиц train_Img, df_train\n",
    "# Nan значения заменим пустыми строками\n",
    "train_d = pd.merge(train_Imgs, df_train,how='left', on=['ImageId','ClassId']) \n",
    "train_d = train_d.fillna('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "y9rzp3og2ijqti7f2de1ka"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50272, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellId": "tyi99tc2275a08vnf8ork"
   },
   "outputs": [],
   "source": [
    "# Изменим структуру таблицы:\n",
    "# каждому ImageId отвечает одна строка, а столбцы - это дефекты различных классов, если дефект какого-либо класса отсутствует, \n",
    "# то в соответствующем столбце - пустое значение\n",
    "train_data = pd.pivot_table(train_d, values='EncodedPixels', index='ImageId',columns='ClassId', aggfunc=np.sum).astype(str)\n",
    "train_data = train_data.reset_index() # add Index column to one level with classID   \n",
    "train_data.columns = ['ImageId','Defect_1','Defect_2','Defect_3','Defect_4']\n",
    "train_data.columns\n",
    "train_data = train_data.replace({'nan':''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellId": "iyy9drjvvqewv27ru0mjic"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12568, 5)\n",
      "12568\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(len(np.unique(train_data.ImageId.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellId": "vcbifoxi4jg3yvcveutqf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Defect_1</th>\n",
       "      <th>Defect_2</th>\n",
       "      <th>Defect_3</th>\n",
       "      <th>Defect_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00031f466.jpg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000418bfc.jpg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000789191.jpg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0007a71bf.jpg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>18661 28 18863 82 19091 110 19347 110 19603 11...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId                                           Defect_1 Defect_2  \\\n",
       "0  0002cc93b.jpg  29102 12 29346 24 29602 24 29858 24 30114 24 3...            \n",
       "1  00031f466.jpg                                                               \n",
       "2  000418bfc.jpg                                                               \n",
       "3  000789191.jpg                                                               \n",
       "4  0007a71bf.jpg                                                               \n",
       "\n",
       "                                            Defect_3 Defect_4  \n",
       "0                                                              \n",
       "1                                                              \n",
       "2                                                              \n",
       "3                                                              \n",
       "4  18661 28 18863 82 19091 110 19347 110 19603 11...           "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellId": "ec39pqx4g4an7zhk1ir6zl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Defect_1</th>\n",
       "      <th>Defect_2</th>\n",
       "      <th>Defect_3</th>\n",
       "      <th>Defect_4</th>\n",
       "      <th>has_defect</th>\n",
       "      <th>number_of_defects</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00031f466.jpg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000418bfc.jpg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000789191.jpg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0007a71bf.jpg</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>18661 28 18863 82 19091 110 19347 110 19603 11...</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId                                           Defect_1 Defect_2  \\\n",
       "0  0002cc93b.jpg  29102 12 29346 24 29602 24 29858 24 30114 24 3...            \n",
       "1  00031f466.jpg                                                               \n",
       "2  000418bfc.jpg                                                               \n",
       "3  000789191.jpg                                                               \n",
       "4  0007a71bf.jpg                                                               \n",
       "\n",
       "                                            Defect_3 Defect_4  has_defect  \\\n",
       "0                                                                       1   \n",
       "1                                                                       0   \n",
       "2                                                                       0   \n",
       "3                                                                       0   \n",
       "4  18661 28 18863 82 19091 110 19347 110 19603 11...                    1   \n",
       "\n",
       "   number_of_defects  \n",
       "0                  1  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Добавим колонки: \n",
    "# has_defect - индификатор наличия дефекта какого-либо класса, \n",
    "has_defect = []\n",
    "for index,row in train_data.iterrows():\n",
    "    if row.Defect_1 or row.Defect_2 or row.Defect_3 or row.Defect_4: \n",
    "        has_defect.append(1)\n",
    "    else:\n",
    "        has_defect.append(0)\n",
    "        \n",
    "train_data[\"has_defect\"] = has_defect \n",
    "number_of_defects=[]\n",
    "for index, row in train_data.iterrows():\n",
    "    i=0\n",
    "    if row.Defect_1:\n",
    "        i=i+1\n",
    "    if row.Defect_2:\n",
    "        i=i+1\n",
    "    if row.Defect_3:\n",
    "        i=i+1\n",
    "    if row.Defect_4:\n",
    "        i=i+1\n",
    "    number_of_defects.append(i)\n",
    "        \n",
    "train_data[\"number_of_defects\"] = number_of_defects  \n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellId": "hsoujj1iy3oas8dj058f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1600, 3)\n",
      "(128, 800, 3)\n"
     ]
    }
   ],
   "source": [
    "f='000418bfc.jpg'\n",
    "data_path = os.path.join(source_folder,'train_images/')\n",
    "orig_img=cv2.imread(os.path.join(data_path, f),cv2.IMREAD_COLOR)\n",
    "o_i=cv2.resize(orig_img,(800,128))\n",
    "print(orig_img.shape)\n",
    "o_im=orig_img.copy()\n",
    "new_img = cv2.resize(o_im,(800,128))\n",
    "print(new_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellId": "6j60789ququ4ioeaahmlqa"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellId": "7f897zub6p6julch2z3woe"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size = 16, subset=\"train\", shuffle=False, preprocess=None, info={},augmentation=False, transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.shuffle = shuffle\n",
    "        self.subset = subset\n",
    "        self.batch_size = batch_size\n",
    "        self.preprocess = preprocess\n",
    "        self.info = info\n",
    "        self.augmentation=augmentation\n",
    "        self.transform=transform\n",
    "        \n",
    "        if self.subset == \"train\":\n",
    "            self.data_path = os.path.join(source_folder,'train_images/')\n",
    "        elif self.subset == \"test\":\n",
    "            self.data_path = os.path.join(source_folder,'test_images/')\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        X = np.empty((self.batch_size,256,1600,3),dtype=np.float32)\n",
    "        y = np.empty((self.batch_size,256,1600,4),dtype=np.int8)\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        for i,f in enumerate(self.df['ImageId'].iloc[indexes]):\n",
    "            self.info[index*self.batch_size+i]=f\n",
    "            X[i,] = Image.open(os.path.join(self.data_path, f)).resize((1600,256))\n",
    "            #X[i,] = np.asarray(X[i,], dtype=np.float32)/255\n",
    "            X[i,] = np.asarray(X[i,], dtype=np.float32)\n",
    "            #X[i,]=cv2.imread(os.path.join(self.data_path, f).format(img),cv2.IMREAD_COLOR)\n",
    "            #X[i,]=cv2.resize(X[i,],(120,120))\n",
    "            if self.subset == 'train': \n",
    "                for j in range(4):\n",
    "                    y[i,:,:,j] = rle_to_mask(self.df['Defect_'+str(j+1)].iloc[indexes[i]])\n",
    "                if self.augmentation==True:\n",
    "                    masks = [y[i,:,:,k] for k in range(4)]\n",
    "                    img = np.array(X[i,])\n",
    "                    transformed = self.transform(image=img.astype('uint8'), masks=masks)\n",
    "                    tr_img = transformed['image']\n",
    "                    X[i,] = cv2.resize(tr_img,(1600,256))\n",
    "                    tr_masks=transformed['masks']\n",
    "                    for j1 in range(4):\n",
    "                        y[i,:,:,j1] = tr_masks[j1] \n",
    "            \n",
    "        if self.preprocess!=None: X = self.preprocess(X)\n",
    "        if self.subset == 'train': return X, y\n",
    "        else: return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellId": "666snvrvrovx09nzqivmrc"
   },
   "outputs": [],
   "source": [
    "class DataGenerator1(keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size = 16, subset=\"train\", shuffle=False, preprocess=None, info={},augmentation=False, transform=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.shuffle = shuffle\n",
    "        self.subset = subset\n",
    "        self.batch_size = batch_size\n",
    "        self.preprocess = preprocess\n",
    "        self.info = info\n",
    "        self.augmentation=augmentation\n",
    "        self.transform=transform\n",
    "        \n",
    "        if self.subset == \"train\":\n",
    "            self.data_path = os.path.join(source_folder,'train_images/')\n",
    "        elif self.subset == \"test\":\n",
    "            self.data_path = os.path.join(source_folder,'test_images/')\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        X = np.empty((self.batch_size,128,800,3),dtype=np.float32)\n",
    "        y = np.empty((self.batch_size,128,800,4),dtype=np.int8)\n",
    "        Xt = np.empty((self.batch_size,128,800,3),dtype=np.float32)\n",
    "        yt = np.empty((self.batch_size,128,800,4),dtype=np.int8)\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        for i,f in enumerate(self.df['ImageId'].iloc[indexes]):\n",
    "            self.info[index*self.batch_size+i]=f\n",
    "            X[i,] = Image.open(os.path.join(self.data_path, f)).resize((800,128))\n",
    "            X[i,] = np.asarray(X[i,], dtype=np.float32)/255\n",
    "            if self.subset == 'train': \n",
    "                for j in range(4):\n",
    "                    y[i,:,:,j] = rle2maskResize(self.df['Defect_'+str(j+1)].iloc[indexes[i]])\n",
    "                if self.augmentation==True:\n",
    "                    masks = [y[i,:,:,k] for k in range(4)]\n",
    "                    img = np.array(X[i,])\n",
    "                    transformed = self.transform(image=img.astype('uint8'), masks=masks)\n",
    "                    tr_img = transformed['image']\n",
    "                    Xt[i,] = cv2.resize(tr_img,(800,128))\n",
    "                    tr_masks=transformed['masks']\n",
    "                    for j1 in range(4):\n",
    "                        yt[i,:,:,j1] = tr_masks[j1]\n",
    "        if self.augmentation==True:    \n",
    "            X = np.concatenate((X,Xt),axis=0)   \n",
    "            y = np.concatenate((y,yt),axis=0)  \n",
    "        if self.preprocess!=None: X = self.preprocess(X)\n",
    "        if self.subset == 'train': return X, y\n",
    "        else: return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellId": "xhep05u50xailjdpcgygyj"
   },
   "outputs": [],
   "source": [
    "transform1 = aug.Compose([\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.01, scale_limit=0.04, rotate_limit=0, p=0.25),\n",
    "        #RandomGamma(p=0.25),\n",
    "        #IAAEmboss(p=0.25),\n",
    "        #Blur(p=0.01, blur_limit = 3),\n",
    "        #OneOf([\n",
    "            #ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "            #GridDistortion(p=0.5),\n",
    "            #OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)                  \n",
    "        #], p=0.3)\n",
    "    ], p = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellId": "jum5fxsgiz8vs68xutxt6"
   },
   "outputs": [],
   "source": [
    "def mask_to_rle(mask):\n",
    "    \"\"\"\n",
    "    params:  mask - numpy array\n",
    "    returns: run-length encoding string (pairs of start & length of encoding)\n",
    "    \"\"\"\n",
    "    \n",
    "    # turn a n-dimensional array into a 1-dimensional series of pixels\n",
    "    # for example:\n",
    "    #     [[1. 1. 0.]\n",
    "    #      [0. 0. 0.]   --> [1. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
    "    #      [1. 0. 0.]]\n",
    "    flat = mask.flatten()\n",
    "    \n",
    "    # we find consecutive sequences by overlaying the mask\n",
    "    # on a version of itself that is displaced by 1 pixel\n",
    "    # for that, we add some padding before slicing\n",
    "    padded = np.concatenate([[0], flat, [0]])\n",
    "    \n",
    "    # this returns the indices where the sliced arrays differ\n",
    "    runs = np.where(padded[1:] != padded[:-1])[0] \n",
    "    # indexes start at 0, pixel numbers start at 1\n",
    "    runs += 1\n",
    "\n",
    "    # every uneven element represents the start of a new sequence\n",
    "    # every even element is where the run comes to a stop\n",
    "    # subtract the former from the latter to get the length of the run\n",
    "    runs[1::2] -= runs[0::2]\n",
    " \n",
    "    # convert the array to a string\n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellId": "3i7lt6krp2rbcbq3uwjx7j"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellId": "3jo834gqu7p5rlhxc8w202"
   },
   "outputs": [],
   "source": [
    "path=source_folder+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellId": "21b1xrquqkombmneybxrwb"
   },
   "outputs": [],
   "source": [
    "def rle_to_mask(lre, shape=(1600, 256)):\n",
    "    '''\n",
    "    params:  rle   - run-length encoding string (pairs of start & length of encoding)\n",
    "             shape - (width,height) of numpy array to return \n",
    "    \n",
    "    returns: numpy array with dimensions of shape parameter\n",
    "    '''    \n",
    "    h, w = shape\n",
    "    if len(lre)>0:# the incoming string is space-delimited\n",
    "        runs = np.asarray([int(run) for run in lre.split(' ')])\n",
    "\n",
    "        # we do the same operation with the even and uneven elements, but this time with addition\n",
    "        runs[1::2] += runs[0::2]\n",
    "        # pixel numbers start at 1, indexes start at 0\n",
    "        runs -= 1\n",
    "\n",
    "        # extract the starting and ending indeces at even and uneven intervals, respectively\n",
    "        run_starts, run_ends = runs[0::2], runs[1::2]\n",
    "\n",
    "        # build the mask\n",
    "        mask = np.zeros(h*w, dtype=np.uint8)\n",
    "        for start, end in zip(run_starts, run_ends):\n",
    "            mask[start:end] = 1\n",
    "    elif len(lre)==0:\n",
    "        mask = np.zeros(h*w, dtype=np.uint8)\n",
    "    # transform the numpy array from flat to the original image shape\n",
    "    return mask.reshape(shape).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "cellId": "qgeyb4nl58hbr6o2iq3poj"
   },
   "outputs": [],
   "source": [
    "def load_img(img_id):\n",
    "    img_dir='train_images' \n",
    "    img = cv2.imread(os.path.join(os.path.join(data_dir,img_dir), img_id))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellId": "ga3dz29b32u11mcod0ik7ef"
   },
   "outputs": [],
   "source": [
    "def show_masked_image(img_id, ax=None, thickness=2):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    img = load_img(img_id)\n",
    "    for i, col in df_train[df_train['ImageId'] == img_id].iterrows():\n",
    "        encoded_pixels = col['EncodedPixels']\n",
    "        label = col['ClassId']\n",
    "        mask = rle_to_mask(encoded_pixels, shape=(1600, 256))\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        img = cv2.drawContours(img, contours, -1, rgb_for_label[label], thickness=thickness)\n",
    "    ax.imshow(img)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "cellId": "5rbbcynrzmyowh4u535jp"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rgb_for_label = {i:v for i, v in enumerate([(0, 192, 12), (0, 185, 241), (114, 0, 218), (249,50,12)], start=1)}\n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(1, 4, figsize=(15, 5))\n",
    "for i in range(0, 4):\n",
    "    ax[i].axis('off')\n",
    "    ax[i].imshow(np.ones((50, 50, 3), dtype=np.uint8) * rgb_for_label[i+1])\n",
    "    ax[i].set_title(\"class color: {}\".format(i+1))\n",
    "fig.suptitle(\"Colors for the classes\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellId": "6s8kbzxfzinkffpm88asc"
   },
   "outputs": [],
   "source": [
    "#%pip install segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellId": "zjv5hlcpvtr0i9odv647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SM_FRAMEWORK=tf.keras\n",
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "%env SM_FRAMEWORK=tf.keras\n",
    "from segmentation_models import Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "b8ipr8twxhn4676u35yjgh"
   },
   "source": [
    "## Modelling main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "b5lot46wweorgxbgx6ed"
   },
   "source": [
    "### Without augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ht4bz4y8bjwbynaf67b86"
   },
   "source": [
    "Possible backbones: ResNet: 'resnet18' 'resnet34' 'resnet50' 'resnet101' 'resnet152'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "q00ev8yohhcmqtu5isbqth",
    "execution_id": "a9081684-a38b-48c7-a724-0c3bae7e4f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 14:35:06.434138: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  4/629 [..............................] - ETA: 41:42:42 - loss: 0.5148 - dice_coef: 0.0233 - iou_coef: 0.0117 - lr: 0.0030"
     ]
    }
   ],
   "source": [
    "#!g1.4\n",
    "# Load U-Net pretrained from ImageNet\n",
    "from keras import backend as K\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Compute Dice Coefficient\n",
    "    \"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def iou_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "\n",
    "BACKBONE = 'resnet101'\n",
    "\n",
    "model = Unet(BACKBONE, input_shape=(256, 1600, 3), classes=4, activation='sigmoid', encoder_weights='imagenet')\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.003)\n",
    "lr_metric = get_lr_metric(opt)\n",
    "model.compile(optimizer=opt , loss='binary_crossentropy', metrics=[dice_coef,  iou_coef, lr_metric])\n",
    "\n",
    "EPOCHES=50 #100\n",
    "\n",
    "# Train and validate the model\n",
    "idx = int(0.8*len(train_data)); print()\n",
    "train_batches = DataGenerator(train_data.iloc[:idx],shuffle=True)\n",
    "valid_batches = DataGenerator(train_data.iloc[idx:])\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, save_best_only=True, mode='max', \n",
    "                             save_weights_only=True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.33,\n",
    "                                   patience=10, verbose=1, mode='max', cooldown=0, min_lr=0.00005)\n",
    "\n",
    "early = EarlyStopping(monitor=\"dice_coef\", mode=\"max\", verbose=2,\n",
    "                      patience=30) # probably needs to be more patient, but kaggle time is limited\n",
    "\n",
    "#model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "   # filepath=checkpoint_filepath,\n",
    "   # save_weights_only=True,\n",
    "    #monitor='val_accuracy',\n",
    "    #mode='max',\n",
    "    #save_best_only=True)\n",
    "\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
    "with tf.device('/GPU'):\n",
    "    history = model.fit(train_batches, validation_data = valid_batches, epochs = EPOCHES, verbose=1,\n",
    "                              callbacks=callbacks_list)\n",
    "# SAVE MODEL\n",
    "\n",
    "model.load_weights(weight_path)\n",
    "model.save('UNET50no_aug_main.h5')\n",
    "\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "#model.load_weights(checkpoint_filepath)\n",
    "# Plot training\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(history.epoch[-1]+1),history.history['val_dice_coef'],label='val_dice_coef')\n",
    "plt.plot(range(history.epoch[-1]+1),history.history['dice_coef'],label='trn_dice_coef')\n",
    "plt.title('Training Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Dice_coef');plt.legend(); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "l1ph94cu5qhpm4yxjev1og"
   },
   "source": [
    "### With augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "jl0zk7v6vns8edsgewnx"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# Load U-Net pretrained from ImageNet\n",
    "from keras import backend as K\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Compute Dice Coefficient\n",
    "    \"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "BACKBONE = 'resnet101'\n",
    "\n",
    "model = Unet(BACKBONE, input_shape=(128, 800, 3), classes=4, activation='sigmoid', encoder_weights='imagenet')\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[dice_coef])\n",
    "EPOCHES=150\n",
    "\n",
    "# Train and validate the model\n",
    "idx = int(0.8*len(train_data)); print()\n",
    "train_batches = DataGenerator(train_data.iloc[:idx],shuffle=True, augmantation=True, transform=transform2)\n",
    "valid_batches = DataGenerator(train_data.iloc[idx:])\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, save_best_only=True, mode='max', \n",
    "                             save_weights_only=True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.33,\n",
    "                                   patience=10, verbose=1, mode='max', cooldown=0, min_lr=0.001)\n",
    "\n",
    "early = EarlyStopping(monitor=\"dice_coef\", mode=\"max\", verbose=2,\n",
    "                      patience=30) # probably needs to be more patient, but kaggle time is limited\n",
    "\n",
    "#model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "   # filepath=checkpoint_filepath,\n",
    "   # save_weights_only=True,\n",
    "    #monitor='val_accuracy',\n",
    "    #mode='max',\n",
    "    #save_best_only=True)\n",
    "\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
    "history = model.fit_generator(train_batches, validation_data = valid_batches, epochs = EPOCHES, verbose=2, callbacks=callbacks_list)\n",
    "# SAVE MODEL\n",
    "\n",
    "model.load_weights(weight_path)\n",
    "model.save('UNET50no_aug1.h5')\n",
    "\n",
    "# The model weights (that are considered the best) are loaded into the model.\n",
    "#model.load_weights(checkpoint_filepath)\n",
    "# Plot training\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(history.epoch[-1]+1),history.history['val_dice_coef'],label='val_dice_coef')\n",
    "plt.plot(range(history.epoch[-1]+1),history.history['dice_coef'],label='trn_dice_coef')\n",
    "plt.title('Training Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Dice_coef');plt.legend(); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "vv3qpttkltrcnucvteqjwu"
   },
   "source": [
    "## Training on augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "yavodxjswrguy4l9loando"
   },
   "source": [
    "We load our previously pretrained model and train it on augmented images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "vup1dflfxspk8sovw9odk",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras import backend as K\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Compute Dice Coefficient\n",
    "    \"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def iou_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "model1 = load_model('UNET50no_aug.h5',custom_objects={'dice_coef':dice_coef})\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=[dice_coef])\n",
    "# Train and validate the model\n",
    "idx = int(0.8*len(train_data)); print()\n",
    "train_batches1 = DataGenerator(train_data.iloc[:idx],shuffle=True,augmentation=True, transform=transform1)\n",
    "valid_batches1 = DataGenerator(train_data.iloc[idx:])\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, save_best_only=True, mode='max', \n",
    "                             save_weights_only=True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.33,\n",
    "                                   patience=10, verbose=1, mode='max', cooldown=0, min_lr=0.001)\n",
    "\n",
    "early = EarlyStopping(monitor=\"dice_coef\", mode=\"max\", verbose=2,\n",
    "                      patience=30) # probably needs to be more patient, but kaggle time is limited\n",
    "\n",
    "#model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "   # filepath=checkpoint_filepath,\n",
    "   # save_weights_only=True,\n",
    "    #monitor='val_accuracy',\n",
    "    #mode='max',\n",
    "    #save_best_only=True)\n",
    "\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n",
    "history1 = model1.fit_generator(train_batches1, validation_data = valid_batches1, epochs = 80, verbose=2, callbacks=callbacks_list)\n",
    "# SAVE MODEL\n",
    "model1.load_weights(weight_path)\n",
    "model1.save('UNET50with_aug.h5')\n",
    "# Plot training\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(range(history1.epoch[-1]+1),history1.history['val_dice_coef'],label='val_dice_coef')\n",
    "plt.plot(range(history1.epoch[-1]+1),history1.history['dice_coef'],label='trn_dice_coef')\n",
    "plt.title('Training Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Dice_coef');plt.legend(); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9j2hnruidfxnr8h54fze"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),input_shape=(120,120,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Conv2D(64,(3,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "model.add(Conv2D(64,(3,3),activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(256,activation=\"relu\"))\n",
    "model.add(Dense(4,activation=\"softmax\"))\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5,min_delta=0.001,restore_best_weights=True)\n",
    "Compiling the model\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer=keras.optimizers.Adam(),\n",
    "             metrics=[\"accuracy\"])\n",
    "#Fitting the model\n",
    "\n",
    "history = model.fit(X_train,y_train,epochs=15,validation_data=(X_test,y_test),batch_size=128,\n",
    "                    verbose=1, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "n6p3vpp4cciawm97aec7pe"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "transform2 = aug.Compose([\n",
    "        HorizontalFlip(p=0.5),\n",
    "        VerticalFlip(p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.01, scale_limit=0.06, rotate_limit=0, p=0.3),\n",
    "        RandomGamma(p=0.25),\n",
    "        #RandomContrast(p=0.5, limit=0.03),\n",
    "        #GaussNoise(var_limit=(10,50), p=0.4),\n",
    "        #RandomBrightness(limit=1,p=1),\n",
    "        #IAAEmboss(p=0.25),\n",
    "        Blur(p=0.5, blur_limit = 3),\n",
    "        #OneOf([\n",
    "            #ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "            #GridDistortion(p=0.5),\n",
    "            #OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5)                  \n",
    "        #], p=0.3)\n",
    "    ], p = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "aug_medium=aug.Compose([\n",
    "     aug.Flip(),\n",
    "     aug.OneOf([\n",
    "            aug.CLAHE(clip_limit=2, p=.5),\n",
    "            aug.IAASharpen(p=.25),\n",
    "            ], p=0.35),\n",
    "     aug.OneOf([\n",
    "         aug.RandomContrast(),\n",
    "         aug.RandomGamma(),\n",
    "         aug.RandomBrightness(),\n",
    "         ], p=0.3),\n",
    "     aug.OneOf([\n",
    "         aug.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "         aug.GridDistortion(),\n",
    "         aug.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n",
    "         ], p=0.3),\n",
    "     aug.ShiftScaleRotate(rotate_limit=12),\n",
    "     aug.OneOf([\n",
    "            aug.GaussNoise(p=.35),\n",
    "            #aug.SaltPepperNoise(level_limit=0.0002, p=.7),\n",
    "            aug.ISONoise(p=.7),\n",
    "            ], p=.5),\n",
    "     aug.Cutout(num_holes=3, p=.25),\n",
    " ], p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "aug_medium1=aug.Compose([\n",
    "     aug.Flip(),\n",
    "     aug.OneOf([\n",
    "         aug.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "         aug.GridDistortion(),\n",
    "         aug.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n",
    "         ], p=0.3),\n",
    "     aug.ShiftScaleRotate(rotate_limit=12, p=0.3),\n",
    "    HorizontalFlip(p=0.4),\n",
    "    VerticalFlip(p=0.4),\n",
    "    Blur(p=0.5, blur_limit = 3)\n",
    " ], p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "yz1bopng0pr035icf825gcu"
   },
   "outputs": [],
   "source": [
    "def show_mask_image(img, mask, label, ax=None, thickness=2):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        \n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    img = cv2.drawContours(img, contours, -1, rgb_for_label[label], thickness=thickness)\n",
    "    ax.imshow(img)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "zgh04n81wonvc5szde4v"
   },
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "ex_gen = DataGenerator(train_data.iloc[:16],shuffle=False,augmentation=True, transform=aug_medium)\n",
    "#val_preds = model.predict(val_gen)\n",
    "\n",
    "\n",
    "def display_mask(i):\n",
    "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "    mask = np.argmax(val_preds[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "    display(img)\n",
    " \n",
    " \n",
    "# Display input image\n",
    "from PIL import Image\n",
    "def visualize(image):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.axis('off')\n",
    "    #plt.imshow(image)\n",
    "    plt.imshow(image.astype('uint8'))\n",
    "    \n",
    "for i, j in ex_gen:\n",
    "    for k in range(5):\n",
    "        filepath=os.path.join(os.path.join(source_folder,'train_images/'), train_data.ImageId[k])\n",
    "        im=Image.open(filepath).resize((1600,256))\n",
    "        im1=np.array(im).astype('uint8')\n",
    "        #print(j.shape)\n",
    "        #print(i.shape)\n",
    "        #visualize(im1)\n",
    "        show_masked_image(train_data.ImageId[k])\n",
    "        #print(i[k].astype(np.uint8))\n",
    "        #visualize(i[k])\n",
    "        #plt.imshow(j[k][:,:,0])\n",
    "        msk=[]\n",
    "        for ii in range(4):\n",
    "            msk.append(np.array(j[k][:,:,ii]).astype(np.uint8))\n",
    "        \n",
    "        #visualize(mask)\n",
    "        Flag=True\n",
    "        for jj, mask in zip([1,2,3,4],msk):\n",
    "            if mask.any()!=0:\n",
    "                show_mask_image(i[k].astype('uint8'), mask, jj, ax=None, thickness=2)\n",
    "                Flag=False\n",
    "        if Flag:\n",
    "            mask0 = np.zeros(256*1600, dtype=np.uint8)\n",
    "            show_mask_image(i[k].astype('uint8'), mask0, 1, ax=None, thickness=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!c1.8\n",
    "ex_gen = DataGenerator(train_data.iloc[:16],shuffle=False,augmentation=True, transform=aug_medium1)\n",
    "#val_preds = model.predict(val_gen)\n",
    "\n",
    "\n",
    "def display_mask(i):\n",
    "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "    mask = np.argmax(val_preds[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "    display(img)\n",
    " \n",
    " \n",
    "# Display input image\n",
    "from PIL import Image\n",
    "def visualize(image):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.axis('off')\n",
    "    #plt.imshow(image)\n",
    "    plt.imshow(image.astype('uint8'))\n",
    "    \n",
    "for i, j in ex_gen:\n",
    "    for k in range(10):\n",
    "        filepath=os.path.join(os.path.join(source_folder,'train_images/'), train_data.ImageId[k])\n",
    "        im=Image.open(filepath).resize((1600,256))\n",
    "        im1=np.array(im).astype('uint8')\n",
    "        #print(j.shape)\n",
    "        #print(i.shape)\n",
    "        #visualize(im1)\n",
    "        show_masked_image(train_data.ImageId[k])\n",
    "        #print(i[k].astype(np.uint8))\n",
    "        #visualize(i[k])\n",
    "        #plt.imshow(j[k][:,:,0])\n",
    "        #visualize(mask)\n",
    "        Flag=True\n",
    "        mask1=np.array(j[k][:,:,0]).astype(np.uint8)\n",
    "        if mask1.any()!=0:\n",
    "            show_mask_image(i[k].astype('uint8'), mask1, 1, ax=None, thickness=2)\n",
    "            Flag=False\n",
    "        mask2=np.array(j[k][:,:,1]).astype(np.uint8)\n",
    "        if mask2.any()!=0:\n",
    "            show_mask_image(i[k].astype('uint8'), mask2, 2, ax=None, thickness=2)\n",
    "            Flag=False\n",
    "        mask3=np.array(j[k][:,:,2]).astype(np.uint8)\n",
    "        if mask3.any()!=0:\n",
    "            show_mask_image(i[k].astype('uint8'), mask3, 3, ax=None, thickness=2)\n",
    "            Flag=False\n",
    "        mask4=np.array(j[k][:,:,3]).astype(np.uint8)\n",
    "        if mask4.any()!=0:\n",
    "            show_mask_image(i[k].astype('uint8'), mask4, 4, ax=None, thickness=2)\n",
    "            Flag=False\n",
    "        if Flag:\n",
    "            mask0 = np.zeros(256*1600, dtype=np.uint8)\n",
    "            show_mask_image(i[k].astype('uint8'), mask0, 1, ax=None, thickness=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "02w6acx1fhi6qxrys4sb54o"
   },
   "source": [
    "We will plot histograms showing the predicted size of each defect mask. We would hope that if an image does not have a particular defect then UNET would not predict a mask (i.e. predict less than 250 pixel mask). This is not the case. When UNET predicts a mask when a defect isn't present, we call that an \"incorrect\" mask. When UNET predicts a mask when a defect is present, we call that a \"correct\" mask. If UNET predicts less than 250 pixels, we will treat that as no mask predicted. Let's compare the distribution of \"incorrect\" versus \"correct\" masks for each defect type.\n",
    "\n",
    "UNET outputs masks using all floating point values between 0 and 1 inclusive. For this classification problem, we need to use only integer 0 and 1. Therefore we must convert mask floating points into integers using a threshold. If pixel>=THRESHOLD then pixel=1 else pixel=0. We will plot histograms for various thresholds below. We will consider all masks with less than 250 pixels as empty masks (where pixel_count = 4 * pixel count on 128x800)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "esaaidyy3as1mdjeifm8r"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "# LOAD MODEL\n",
    "from keras import backend as K\n",
    "%enable_full_walk\n",
    "from tensorflow.keras.models import load_model\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Compute Dice Coefficient\n",
    "    \"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "model = load_model('UNET50no_aug.h5',custom_objects={'dice_coef':dice_coef})\n",
    "\n",
    "# PREDICT 1 BATCH TEST DATASET\n",
    "# test['ImageId'] = test['ImageId_ClassId'].map(lambda x: x.split('_')[0])\n",
    "test_batches = DataGenerator(df_sample,subset='test',shuffle=False,  batch_size = 2)\n",
    "test_preds = model.predict(test_batches,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "qptfs78qgcgylbhcjx1bx"
   },
   "outputs": [],
   "source": [
    "def mask_class_def(i):   \n",
    "    mask_max = np.where(np.max(test_preds[i], axis=-1)<0.88,0,1)\n",
    "    mask_argmax = np.argmax(test_preds[i], axis=-1)+1\n",
    "    mask0=np.multiply(mask_max,mask_argmax)\n",
    "    return mask0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "tao3ojjs9y99ykkm1p7e"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "DF=pd.DataFrame()\n",
    "List=[]\n",
    "print(test_preds.shape)\n",
    "for i in range(0,df_sample.shape[0]):\n",
    "    df=pd.DataFrame()\n",
    "    imid=df_sample.ImageId.values[i]\n",
    "    mask=mask_class_def(i)\n",
    "    rles=[]\n",
    "    for k in range(1,5):\n",
    "        rles.append(mask_to_rle(np.where(mask==k,1,0)))\n",
    "    df['ImageId_ClassId']=pd.Series([str(imid)+'_'+str(m) for m in range(1,5)])\n",
    "    df['EncodedPixels']=rles\n",
    "    DF=pd.concat([DF, df])\n",
    "DF.to_csv('submission3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "1jg91ar6ca7y7l6bwvdveb"
   },
   "outputs": [],
   "source": [
    "DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dhntkv5k5kihibo8xu1r2p"
   },
   "outputs": [],
   "source": [
    "DF.EncodedPixels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dnejhztrwl81z8gezy10qr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notebookId": "fd538ebb-3ac1-4b21-be5b-4ad171fc989d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
